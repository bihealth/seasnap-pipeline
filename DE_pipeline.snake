## SeA-SnaP differential expression pipeline for RNA-seq analysis
## version: 1.0
## author: J.P.Pett (patrick.pett@bihealth.de)

#TODO: replace config value check by built-in? https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#validation

import os, re, yaml, textwrap, pandas as pd
from collections import OrderedDict
from time import asctime, localtime, time
from pathlib import Path
from snakemake.utils import report, format as snakemake_format
from tools.pipeline_tools import DEPipelinePathHandler, ReportTool

yaml.add_representer(OrderedDict, lambda dumper, data: dumper.represent_dict(dict(data)))

SNAKEDIR  = Path(workflow.current_basedir)
SNAKEFILE = workflow.snakefile
SCRIPTDIR = str(SNAKEDIR / "external_scripts")

# assemble config
configfile: str(SNAKEDIR / "defaults" / "DE_config_defaults.yaml")
configfile: "DE_config.yaml"
if config["organism_defaults"]:
	configfile: str(SNAKEDIR / "defaults" / config["organism_defaults"])
	configfile: "DE_config.yaml"

# create path handler
conf_ranges = str(SNAKEDIR / "defaults" / "DE_config_ranges.yaml")
test_config = conf_ranges if config["pipeline_param"]["test_config"] else None
pph = DEPipelinePathHandler(workflow, test_config)

# exclude symbols '.' and '/' from wildcards
wildcard_constraints: 
	sample="[^./]+"


R_SESSION_INFO = r"""
cat("########################### session info ############################","\n")
print(sessionInfo())
cat("#####################################################################","\n\n")
"""

onstart:
	# draw a dag
	dag_file = pph.file_path(step="pipeline_report", extension="rule_execution.png", contrast="all")
	os.makedirs(os.path.dirname(dag_file), exist_ok=True)
	shell("snakemake --quiet --snakefile {} --rulegraph | dot -Tpng > {}".format(SNAKEFILE, dag_file))
	# info about the pipeline run
	info_file = pph.file_path(step="pipeline_report", extension="summary.csv", contrast="all")
	os.makedirs(os.path.dirname(info_file), exist_ok=True)
	shell("snakemake --quiet --snakefile {} --summary | sed 's/\t/, /g' > {}".format(SNAKEFILE, info_file))
	# save merged config
	config_file = pph.file_path(step="pipeline_report", extension="yaml", contrast="all")
	with open(config_file, "w") as f: yaml.dump(config, f, default_flow_style=False)


##-------------------- starting point ----------------------------------------------------------------------

def get_inputs_all():
	#contrasts
	inputs = pph.expand_path(step="contrast", extension="rds")
	
	#functional annotation
	inputs.append(pph.expand_path(step = "goseq", extension = "go.rds",   if_set = dict(goseq=True) ))
	inputs.append(pph.expand_path(step = "goseq", extension = "kegg.rds", if_set = dict(goseq=True) ))
	inputs.append(pph.file_path(step = "annotation", extension = "rds", contrast="all"))

	inputs.append(pph.expand_path(step = "cluster_profiler", extension = "rds", if_set = dict(cluster_profiler=dict(run=True)) ))
	inputs.append(pph.file_path(step = "tmod_dbs", extension = "rds", contrast="all"))
	inputs.append(pph.file_path(step = "tmod_pca", extension = "rds", contrast="all"))
	inputs.append(pph.expand_path(step = "tmod", extension = "rds", if_set = dict(tmod=True)))
	
	return inputs


shell("rm -f {}".format(pph.file_path(step="pipeline_report", extension="report.html", contrast="all")))

rule all:
	input:
		get_inputs_all(),
		# html generation does not work
		#pph.file_path(step="report_html", extension="html", contrast="all"),
		pph.file_path("report", "Rmd", contrast = "all")
	output:
		html = pph.file_path(step="pipeline_report", extension="report.html", contrast="all")
	run:
		loctime = asctime(localtime(time()))
		rule_execution = pph.file_path("pipeline_report", "rule_execution.png", contrast="all")
		summary        = pph.file_path("pipeline_report", "summary.csv",        contrast="all")
		version_info   = pph.file_path("pipeline_report", "version_info.txt",   contrast="all")
		conda_info     = pph.file_path("pipeline_report", "conda_info.txt",     contrast="all")
		dag = rule_execution.split("/")[-1]
		shell("conda list > {}".format(version_info))
		shell("conda info > {}".format(conda_info))
		report("""
		=======================
		RNAseq mapping pipeline
		=======================
		
		**Finished: {loctime}**
		
		.. image:: {dag}
		
		File status at pipeline start:
		==============================
		
		.. csv-table::
			:file: {summary}
			
		Version info:
		=============
		
		.. include:: {version_info}
			:literal:
		
		Conda info:
		===========
		
		.. include:: {conda_info}
			:literal:
		
		""", output.html, graph = rule_execution, table = summary)


rule export:
	input:
		get_inputs_all()
	run:
		pph.export()

	
##-------------------- make TxDb -----------------------------------------------------------------------------

rule TxDb_from_GTF:
	""" make a TxDb object from GTF """
	input:
		gtf = config["organism"]["files"]["gtf"]
	output:
		pph.file_path(step = "TxDb_from_GTF", extension = "sqlite", contrast = "all")
	log:
		out = pph.file_path("TxDb_from_GTF", "output.log", contrast = "all", log=True)
	run:
		genus = config["organism"]["genus"]
		taxon = config["organism"]["taxon"]
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(GenomicFeatures)
		
		{R_SESSION_INFO}
		
		#----- make transcript to gene table
		TxDb <- makeTxDbFromGFF("{input.gtf}", format="gtf", organism="{genus}", taxonomyId={taxon})
		
		#----- save Db
		saveDb(TxDb, "{output}")
		""")
		
		script_file = pph.log(log.out, snakemake_format(script), step="TxDb_from_GTF", extension="R", contrast = "all", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")


##-------------------- import -------------------------------------------------------------------------------

DESIGN = config["experiment"]["design_formula"]


rule import_gene_counts:
	""" collect count files (STAR) and build count matrix """
	input:
		samples = config["experiment"]["covariate_file"]["star"]
	output:
		rds=pph.file_path("import_gene_counts", "rds", contrast = "all"),
		xlsx=pph.file_path("import_gene_counts", "xlsx", contrast = "all")
	log:
		out = pph.file_path("import_gene_counts", "output.log", contrast = "all", log=True)
	run:
		column_filter = ""
		if config["filters"]["experiment_whitelist"]:
			column_filter += "&".join("is.element(samples${},{})".format( k, "c({})".format(",".join('"'+l+'"' for l in v)) ) 
								for k,v in config["filters"]["experiment_whitelist"].items())
		if config["filters"]["experiment_blacklist"]:
			column_filter += "&".join("!is.element(samples${},{})".format( k, "c({})".format(",".join('"'+l+'"' for l in v)) ) 
								for k,v in config["filters"]["experiment_blacklist"].items())
		if column_filter == "":
			column_filter = "TRUE"
		
		column_lvl = config["experiment"]["columns"] if "columns" in config["experiment"] else {}
		level_list = "list({})".format(",".join("{}=c({})".format( n, ",".join('"'+s+'"' for s in l) ) for n,l in column_lvl.items()))
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(DESeq2)
    library(writexl)
		
		{R_SESSION_INFO}

    #----- variables
    sample_f  <- "{input.samples}"
    output_xlsx <- "{output.xlsx}"
    output_rds  <- "{output.rds}"
		
		#----- import data
		sample_df <- read.table(sample_f, header=TRUE)
		files     <- as.character(sample_df$filename); print(files)
		samples   <- as.character(sample_df$label)

		#----- merge count files from star
		df_list   <- lapply(1:length(files), function(i) as.data.frame(read.csv(files[i], skip=4, sep="\t", 
		                                                                        col.names=c("ID", samples[i]), check.names=FALSE, header=FALSE)))
		count_dat <- Reduce(function(...) merge(..., all=TRUE, by="ID"), df_list); rownames(count_dat) <- count_dat$ID
		
		#----- change levels
		level_cols <- {level_list}
		for (col in names(level_cols)) {{
			sample_df[,col] <- factor(sample_df[,col], levels = level_cols[[col]])
		}}

		#----- import counts in DESeq2
		rownames(sample_df) <- sample_df$label; count_dat <- count_dat[, rownames(sample_df)]
		dds <- DESeqDataSetFromMatrix(countData = as.matrix(count_dat), colData = sample_df, design = {DESIGN})
		
		#----- save as rds
		saveRDS(dds, file=output_rds)

    #----- export counts as XLSX
    colnames(count_dat) <- sample_df$label
    write_xlsx(count_dat, path=output_xlsx)
		""")
		
		script_file = pph.log(log.out, snakemake_format(script), step="import_gene_counts", extension="R", contrast = "all", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")

rule import_featurecounts:
	""" collect count files (featurecounts) and build count matrix """
	input:
		samples = config["experiment"]["covariate_file"]["star"]
	output:
		pph.file_path("import_featurecounts", "rds", contrast = "all")
	log:
		out = pph.file_path("import_featurecounts", "output.log", contrast = "all", log=True)
	run:
		column_filter = ""
		if config["filters"]["experiment_whitelist"]:
			column_filter += "&".join("is.element(samples${},{})".format( k, "c({})".format(",".join('"'+l+'"' for l in v)) ) 
								for k,v in config["filters"]["experiment_whitelist"].items())
		if config["filters"]["experiment_blacklist"]:
			column_filter += "&".join("!is.element(samples${},{})".format( k, "c({})".format(",".join('"'+l+'"' for l in v)) ) 
								for k,v in config["filters"]["experiment_blacklist"].items())
		if column_filter == "":
			column_filter = "TRUE"
								
		column_lvl = config["experiment"]["columns"] if "columns" in config["experiment"] else {}
		level_list  = "list({})".format(",".join("{}=c({})".format( n, ",".join('"'+s+'"' for s in l) ) for n,l in column_lvl.items()))
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(DESeq2)
		
		{R_SESSION_INFO}
		
		#----- import data
		sample_df <- read.table("{input.samples}", header=TRUE)
		files     <- as.character(sample_df$filename); print(files)
		samples   <- as.character(sample_df$label)

		#----- merge count files
		read_fc <- function(fn, sn) read.csv(fn, comment="#", sep="\t", colClasses=c("character", rep("NULL", 5), "numeric"),
		                                             col.names=c("ID", rep("dummy", 5), sn), check.names=FALSE)
		df_list   <- lapply(1:length(files), function(i) as.data.frame(read_fc(files[i], samples[i])))
		count_dat <- Reduce(function(...) merge(..., all=TRUE, by="ID"), df_list); rownames(count_dat) <- count_dat$ID
		
		#----- change levels
		level_cols <- {level_list}
		for (col in names(level_cols)) {{
			sample_df[,col] <- factor(sample_df[,col], levels = level_cols[[col]])
		}}

		#----- import counts in DESeq2
		rownames(sample_df) <- sample_df$label; count_dat <- count_dat[, rownames(sample_df)]
		dds <- DESeqDataSetFromMatrix(countData = as.matrix(count_dat), colData = sample_df, design = {DESIGN})
		
		#----- save as rds
		saveRDS(dds, file="{output}")
		""")
		
		script_file = pph.log(log.out, snakemake_format(script), step="import_featurecounts", extension="R", contrast = "all", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")
	
rule import_sf:
	""" collect sf files (Salmon) and build count matrix """
	input:
		txdb    = pph.file_path("TxDb_from_GTF", "sqlite", contrast = "all"),
		samples = config["experiment"]["covariate_file"]["salmon"]
	output:
		pph.file_path("import_sf", "rds", contrast = "all")
	log:
		out = pph.file_path("import_sf", "output.log", contrast = "all", log=True)
	run:
		column_filter = ""
		if config["filters"]["experiment_whitelist"]:
			column_filter += "&".join("is.element(sample_df${},{})".format( k, "c({})".format(",".join('"'+l+'"' for l in v)) ) 
								for k,v in config["filters"]["experiment_whitelist"].items())
		if config["filters"]["experiment_blacklist"]:
			column_filter += "&".join("!is.element(sample_df${},{})".format( k, "c({})".format(",".join('"'+l+'"' for l in v)) ) 
								for k,v in config["filters"]["experiment_blacklist"].items())
		if column_filter == "":
			column_filter = "TRUE"
								
		column_lvl = config["experiment"]["columns"] if "columns" in config["experiment"] else {}
		level_list = "list({})".format(",".join("{}=c({})".format( k, ",".join('"'+l+'"' for l in v) ) for k,v in column_lvl.items()))
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(DESeq2)
		library(tximport)
		library(readr)
		library(AnnotationDbi)
		
		{R_SESSION_INFO}
		
		#----- make transcript to gene table
		TxDb    <- loadDb(file = "{input.txdb}")
		k       <- keys(TxDb, keytype = "TXNAME")
		tx2gene <- select(TxDb, k, "GENEID", "TXNAME")
		
		#----- use tximport on input files
		sample_df  <- read.table("{input.samples}", header=TRUE)
		sample_df  <- sample_df[{column_filter},]
		files      <- as.character(sample_df$filename); print(files)
		txi        <- tximport(files, type = "salmon", tx2gene = tx2gene)
		
		#----- change levels
		level_cols <- {level_list}
		for (col in names(level_cols)) {{
			sample_df[,col] <- factor(sample_df[,col], levels = level_cols[[col]])
		}}

		#----- import txi in DESeq2
		rownames(sample_df) <- sample_df$label
		dds <- DESeqDataSetFromTximport(txi, colData = sample_df, design = {DESIGN})
		
		#----- save as rds
		saveRDS(dds, file="{output}")
		""")
		script_file = pph.log(log.out, snakemake_format(script), step="import_sf", extension="R", contrast = "all", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")


##-------------------- DESeq2 -------------------------------------------------------------------------------

rule DESeq2:
	""" run DESeq2 on a count matrix """
	input:
		pph.choose_input(choice_name = "mapping", options = [
		dict(step = "import_gene_counts",   extension = "rds", contrast = "all"),
		dict(step = "import_featurecounts", extension = "rds", contrast = "all"),
		dict(step = "import_sf",            extension = "rds", contrast = "all")
		])
	output:
		dds           = pph.file_path("DESeq2", "deseq2.rds", contrast = "all"),
		rld_blind     = pph.file_path("DESeq2", "rld.blind.rds", contrast = "all"),
		rld_model     = pph.file_path("DESeq2", "rld.model.rds", contrast = "all"),
		rld_blind_csv = pph.file_path("DESeq2", "rld.blind.csv", contrast = "all"),
		rld_model_csv = pph.file_path("DESeq2", "rld.model.csv", contrast = "all")
	log:
		out = pph.file_path("DESeq2", "output.log", contrast = "all", log=True)
	params:
		coef_names = pph.file_path("DESeq2", "coef_names.txt", contrast = "all"),
		rld_names  = pph.file_path("DESeq2", "rld.model.csv", contrast = "all"),
		annot_pkg  = config["organism"]["R"]["annotations"]
	run:
		count_threshold       = config["filters"]["low_counts"]
                normalized_expression = config["normalization"]["normalized_expression"]
		script = textwrap.dedent(r"""
		#----- import packages
		library(DESeq2)
		library(AnnotationDbi)
		library({params.annot_pkg})
		
		{R_SESSION_INFO}
		
		#----- load DESeqDataSet
		dds <- readRDS("{input}")
		
		#----- pre-filtering
		keep <- rowSums(counts(dds)) >= {count_threshold}
		dds  <- dds[keep,]
		
		#----- DESeq2
		dds <- DESeq(dds, betaPrior=FALSE, test="Wald")

		# ----- Normalised expression values
		rld_blind <- switch("{normalized_expression}",
			"rld"=rlog(dds, blind=TRUE),
			"vst"=vst(dds, blind=TRUE)
		)
		rld_model <- switch("{normalized_expression}",
			"rld"=rlog(dds, blind=FALSE),
			"vst"=vst(dds, blind=FALSE)
		)

		#----- save as rds
		saveRDS(dds, file="{output.dds}")
		saveRDS(rld_blind, file="{output.rld_blind}")
		saveRDS(rld_model, file="{output.rld_model}")

		#----- save other files
		writeLines(resultsNames(dds), "{params.coef_names}")
		write.table(data.frame(
			gene_id=rownames(rld_model),
			assay(rld_model),
			symbol=mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(dds)), 
			              column="SYMBOL", keytype="ENSEMBL", multiVals="first"),
			entrez=mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(dds)), 
			              column="ENTREZID", keytype="ENSEMBL", multiVals="first"),
			check.names=FALSE
			), file="{output.rld_model_csv}", sep="\t", col.names=TRUE, row.names=FALSE, quote=FALSE)
		write.table(data.frame(
			gene_id=rownames(rld_blind),
			assay(rld_blind),
			symbol=mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(dds)), 
			              column="SYMBOL", keytype="ENSEMBL", multiVals="first"),
			entrez=mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(dds)), 
			              column="ENTREZID", keytype="ENSEMBL", multiVals="first"),
			check.names=FALSE
			), file="{output.rld_blind_csv}", sep="\t", col.names=TRUE, row.names=FALSE, quote=FALSE)
		""")
		script_file = pph.log(log.out, snakemake_format(script), step="DESeq2", extension="R", contrast = "all", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")
		
		
##-------------------- contrasts ----------------------------------------------------------------------------

rule contrast:
	""" make a contrast """
	input:
		pph.file_path("DESeq2", "deseq2.rds", contrast = "all")
	output:
		rds = pph.file_path("contrast", "rds"),
		csv = pph.file_path("contrast", "csv")
	log:
		out           = pph.file_path("contrast", "output.log",  log=True),
		contrast_yaml = pph.file_path("contrast", "contrast.yaml", log=True)
	params:
		ma_plot    = pph.file_path("contrast", "ma.pdf"),
		count_plot = pph.file_path("contrast", "counts.pdf"),
		annot_pkg  = config["organism"]["R"]["annotations"]
	run:
		contrast = pph.get_contrast(wildcards.contrast)
		with open(log.contrast_yaml, "w") as f: yaml.dump(contrast, f, default_flow_style=False)
		
		# variants of contrast definition
		contrast_column, contrast_ref = "group", ""
		if "coef" in contrast:
			contrast_type          = "coef"
			arg_val                = '="{}"'.format(contrast["coef"])
			contrast_def, lfc_def  = "name" + arg_val, "coef" + arg_val
		elif "ratio" in contrast:
			contrast_type, ratio   = "ratio", contrast["ratio"]
			contrast_column, contrast_num, contrast_ref = ratio["column"], ratio["numerator"], ratio["denominator"]
			contrast_def = lfc_def = 'contrast=c("{}", "{}", "{}")'.format(contrast_column, contrast_num, contrast_ref)
		elif "vector" in contrast:
			contrast_type          = "vector"
			contrast_def = lfc_def = 'contrast=c({})'.format(", ".join(map(str, contrast["vector"])))
		else:
			raise ValueError("Error in contrast: no valid contrast definition! must be one of {}".format(["coef:", "ratio:", "vector:"]))
		
		# parameters
		cutoff_FDR           =           contrast["max_p_adj"]
		lfcThreshold         =           contrast["results_parameters"]["lfcThreshold"]
		altHypothesis        =           contrast["results_parameters"]["altHypothesis"]
		independentFiltering = "TRUE" if contrast["results_parameters"]["independentFiltering"] else "FALSE"
		lfc_shrink_type      =           contrast["lfcShrink_parameters"]["type"]
		rank_by              =           contrast["ranking_by"]
		rank_order           =           contrast["ranking_order"].replace("x", "res$"+rank_by)
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(DESeq2)
		library(AnnotationDbi)
		library({params.annot_pkg})
		
		{R_SESSION_INFO}
		
		#----- load DESeqDataSet
		dds <- readRDS("{input}")
		
		#----- contrast
		if ("{contrast_type}" == "ratio") {{
			dds${contrast_column} = relevel(dds${contrast_column}, ref="{contrast_ref}")
			dds <- nbinomWaldTest(dds)
		}}
		res <- results(dds, {contrast_def}, alpha={cutoff_FDR}, lfcThreshold={lfcThreshold}, 
		               altHypothesis="{altHypothesis}", independentFiltering={independentFiltering})

		#----- log-fold-change shrinkage
		#res$lfcShrunk   <- res$log2FoldChange
		#res$lfcShrunkSE <- res$lfcSE
		if ("{lfc_shrink_type}"!="none"){{
			lfc <- lfcShrink(dds, {lfc_def}, lfcThreshold={lfcThreshold}, type="{lfc_shrink_type}")
			res$log2FoldChange_orig <- res$log2FoldChange
			res$log2FoldChange      <- lfc$log2FoldChange
			res$lfcSE_orig          <- res$lfcSE
			res$lfcSE               <- lfc$lfcSE
		}}
		
		#----- ranking
		res <- res[order({rank_order}),]
		
		#----- annotation [BEWARE- the in case on multiple mapping, the first entry will be selected]
		res$symbol <- mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(res)), 
		                     column="SYMBOL", keytype="ENSEMBL", multiVals="first")
		res$entrez <- mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(res)), 
		                     column="ENTREZID", keytype="ENSEMBL", multiVals="first")
		
		#----- save as rds & csv
		saveRDS(res, file="{output.rds}")
		write.table(data.frame(gene_id=rownames(res), res), file="{output.csv}", sep="\t", col.names=TRUE, row.names=FALSE, quote=FALSE)
		""")
		
		script_file = pph.log(log.out, snakemake_format(script), step="contrast", extension="R", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")


##-------------------- contrasts ----------------------------------------------------------------------------
## Same as contrasts, but saves all results from DESeq2
## everything named contrasts_full for consistency

rule contrasts_full:
	""" make all contrasts (full version) """
	input:
		pph.file_path("DESeq2", "deseq2.rds", contrast = "all")
	output:
		rds = pph.file_path("contrasts_full", "rds"),
		csv = pph.file_path("contrasts_full", "csv")
	log:
		out           = pph.file_path("contrasts_full", "output.log",  log=True),
	params:
		annot_pkg  = config["organism"]["R"]["annotations"]
	run:
		contrast = pph.get_contrast(wildcards.contrast)
		#with open(log.contrast_yaml, "w") as f: yaml.dump(contrast, f, default_flow_style=False)
		
		# variants of contrast definition
		contrast_column, contrast_ref = "group", ""
		if "coef" in contrast:
			contrast_type          = "coef"
			arg_val                = '="{}"'.format(contrast["coef"])
			contrast_def, lfc_def  = "name" + arg_val, "coef" + arg_val
		elif "ratio" in contrast:
			contrast_type, ratio   = "ratio", contrast["ratio"]
			contrast_column, contrast_num, contrast_ref = ratio["column"], ratio["numerator"], ratio["denominator"]
			contrast_def = lfc_def = 'contrast=c("{}", "{}", "{}")'.format(contrast_column, contrast_num, contrast_ref)
		elif "vector" in contrast:
			contrast_type          = "vector"
			contrast_def = lfc_def = 'contrast=c({})'.format(", ".join(map(str, contrast["vector"])))
		else:
			raise ValueError("Error in contrast: no valid contrast definition! must be one of {}".format(["coef:", "ratio:", "vector:"]))
		
		# parameters
		cutoff_FDR           =           contrast["max_p_adj"]
		lfcThreshold         =           contrast["results_parameters"]["lfcThreshold"]
		altHypothesis        =           contrast["results_parameters"]["altHypothesis"]
		independentFiltering = "TRUE" if contrast["results_parameters"]["independentFiltering"] else "FALSE"
		lfc_shrink_type      =           contrast["lfcShrink_parameters"]["type"]
		rank_by              =           contrast["ranking_by"]
		rank_order           =           contrast["ranking_order"].replace("x", "res$"+rank_by)
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(AnnotationDbi)
		library(DESeq2)
		library({params.annot_pkg})
		
		{R_SESSION_INFO}

    
		
		#----- load DESeqDataSet
		dds <- readRDS("{input}")
		
		#----- contrast
		if ("{contrast_type}" == "ratio") {{
			dds${contrast_column} = relevel(dds${contrast_column}, ref="{contrast_ref}")
			dds <- nbinomWaldTest(dds)
		}}
		res <- results(dds, {contrast_def}, # alpha={cutoff_FDR}, lfcThreshold={lfcThreshold}, 
		               altHypothesis="{altHypothesis}") #, independentFiltering={independentFiltering})

		#----- log-fold-change shrinkage
		#res$lfcShrunk   <- res$log2FoldChange
		#res$lfcShrunkSE <- res$lfcSE
		if ("{lfc_shrink_type}"!="none"){{
			lfc <- lfcShrink(dds, {lfc_def}, # lfcThreshold={lfcThreshold}, 
				type="{lfc_shrink_type}")
			res$log2FoldChange_orig <- res$log2FoldChange
			res$log2FoldChange      <- lfc$log2FoldChange
			res$lfcSE_orig          <- res$lfcSE
			res$lfcSE               <- lfc$lfcSE
		}}
		
		#----- ranking
		res <- res[order({rank_order}),]
		
		#----- annotation [BEWARE- the in case on multiple mapping, the first entry will be selected]
		res$symbol <- mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(res)), 
		                     column="SYMBOL", keytype="ENSEMBL", multiVals="first")
		res$entrez <- mapIds({params.annot_pkg}, keys=sub("\\.[0-9]+$", "", row.names(res)), 
		                     column="ENTREZID", keytype="ENSEMBL", multiVals="first")
		
		#----- save as rds & csv
		saveRDS(res, file="{output.rds}")
		write.table(data.frame(gene_id=rownames(res), res), file="{output.csv}", sep="\t", col.names=TRUE, row.names=FALSE, quote=FALSE)
		""")
		
		script_file = pph.log(log.out, snakemake_format(script), step="contrasts_full", extension="R", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")



## ------------------------ annotation ---------------------------

rule annotation:
	""" Adding annotation information """
	input:
		# we use dds as this is the most reliable source of the primary IDs
		dds       = pph.file_path("DESeq2", "deseq2.rds", contrast = "all")
	output:
		res = pph.file_path(step = "annotation", extension = "rds", contrast="all"),
		csv = pph.file_path(step = "annotation", extension = "csv", contrast="all")
	log:
		out = pph.file_path(step = "annotation", extension = "output.log", log=True, contrast="all")
	run:
		config_file = pph.file_path(step="pipeline_report", extension="yaml", contrast="all")
		script = textwrap.dedent(r"""
			library(AnnotationDbi)
			{R_SESSION_INFO}

			res.file <- "{output.res}"
			res.csv  <- "{output.csv}"
			script.d <- "{SCRIPTDIR}"
			conf.f   <- "{config_file}"
      dds      <- "{input.dds}"
			 
			# ----------------------------------------
			# No Snakemake wildcards beyond this point
			# ----------------------------------------
			library(orthomapper)

			config <- yaml::yaml.load_file(conf.f)

			gene_ids <- rownames(readRDS(dds))

			res <- data.frame(PrimaryID=gene_ids, stringsAsFactors=FALSE)
			res$ENSEMBL <- sub("\\..*$", "", res$PrimaryID)

			annot <- orthomapper::entrez_annotate(res$ENSEMBL, taxon=config$organism$taxon,
				keytype="ENSEMBL", column=c("SYMBOL", "ENTREZID", "REFSEQ", "GENENAME"))

		  if(!all(annot[,1] == res$ensembl)) 
				stop("object returned by entrez_annotate() does not match query")

			## no need to duplicate columns
			res <- cbind(res, annot[, -1, drop=FALSE])

			write.csv(res, row.names=FALSE, file=res.csv)
	    saveRDS(res, file=res.file)
		""")

		script_file = pph.log(log.out, snakemake_format(script), step="annotation", extension="R", contrast="all", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")





## ------------------------ preparation of the tmod databases ---------------------------

rule tmod_dbs:
	""" Preparing tmod databases """
	input:
		#rds = pph.file_path("contrasts_full", "rds"),
		annotation = pph.file_path(step="annotation", extension="rds", contrast="all")
	output:
		res = pph.file_path(step = "tmod_dbs", extension = "rds", contrast="all"),
		map = pph.file_path(step = "tmod_dbs", extension = "mapping.rds", contrast="all")
	log:
		out = pph.file_path(step = "tmod_dbs", extension = "output.log", log=True, contrast="all")
	run:
		config_file = pph.file_path(step="pipeline_report", extension="yaml", contrast="all"),
		script = textwrap.dedent(r"""
			{R_SESSION_INFO}

			res.file <- "{output.res}"
			map.file <- "{output.map}"
			script.d <- "{SCRIPTDIR}"
			conf.f   <- "{config_file}"
			annot.f  <- "{input.annotation}"
			 
			## ------------------------------
      ## no wildcards beyond this point
			## ------------------------------

			config <- yaml::yaml.load_file(conf.f)
			source(file.path(script.d, "tmod_functions.R"))

			## read the necessary files
			## create the databases
			dbs <- process_dbs(config$tmod)
			saveRDS(dbs, file=res.file)

			## provide mapping for the databases
			map <- get_mapping(config, config$tmod$databases, annot.f)
			saveRDS(map, file=map.file)
		""")

		script_file = pph.log(log.out, snakemake_format(script), step="tmod_dbs", extension="R", contrast="all", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")




## ------------------------ gene set enrichments with tmod ---------------------------

rule tmod:
	""" Gene set enrichment with tmod and MSigDB """
	input:
		rds = pph.file_path("contrasts_full", "rds"),
		dbs = pph.file_path(step = "tmod_dbs", extension = "rds", contrast="all"),
		map = pph.file_path(step = "tmod_dbs", extension = "mapping.rds", contrast="all")
	output:
		tmod_res  = pph.file_path(step = "tmod", extension = "rds"),
		tmod_gl   = pph.file_path(step = "tmod", extension = "gl.rds"),
		tmod_xlsx = pph.file_path(step = "tmod", extension = "xlsx")
	log:
		out = pph.file_path(step = "tmod", extension = "output.log", log=True)
	run:
		config_file = pph.file_path(step="pipeline_report", extension="yaml", contrast="all")
		script = textwrap.dedent(r"""
			{R_SESSION_INFO}

			script.d <- "{SCRIPTDIR}"
			conf.f   <- "{config_file}"
			de.input <- "{input.rds}"
			map.f    <- "{input.map}"
			dbs.f    <- "{input.dbs}"
			res.file <- "{output.tmod_res}"
			res.xlsx <- "{output.tmod_xlsx}"
			res.gl.f <- "{output.tmod_gl}"
			 
			## ------------------------------
      ## no wildcards beyond this point
			## ------------------------------
			library(DESeq2)
			library(tmod)
			library(writexl)
			
			config <- yaml::yaml.load_file(conf.f)
			source(file.path(script.d, "tmod_functions.R"))

			dbs <- readRDS(dbs.f)
			db.map <- readRDS(map.f)
			de.res <- readRDS(de.input)
			foo <- rownames(de.res)
			if(is.null(foo)) stop("foo is NULL!")

			## create ordered gene lists for every database
			message(paste0("Starting tmod for contrast ", de.input))
			ordered_genelist <- get_ordered_genelist(de.res, config)
			genelists <- lapply(dbs, map_genelists, ordered_genelist, db.map)
			saveRDS(genelists, file=res.gl.f)
			
			## run tmod using each of the predefined databases
			res <- lapply(dbs, run_tmod, config, genelists, db.map)

			## res is a list with an element for each db
			## each element for a db is a list with an element for each
			## of the sorting keys
			## each of these elements contains objects `res` (df with results)
			## and `gl` (genelist used to generate the results) 
			saveRDS(res, file=res.file)
			message("rds saved")

			## shorten results for the human readable XLSX file

			## filter the results
			res.short <- lapply(res, function(x) lapply(x, reformat_res, pval.thr=.05, auc.thr=.55))

			## res.short still has two levels, one for the databases, another one
			## for sorting keys, so we need to unlist – one level only
			res.short <- unlist(res.short, recursive=FALSE)
			write_xlsx(res.short, path=res.xlsx)
		""")

		script_file = pph.log(log.out, snakemake_format(script), step="tmod", extension="R", **wildcards)

		# run R
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")

##-------------------- multivariate tmod ----------------------------------------------------------------

rule tmod_pca:
	""" Gene set enrichment of PCA with tmod and MSigDB """
	input:
		dds = pph.file_path("DESeq2", "deseq2.rds", contrast = "all"),
		dbs = pph.file_path(step = "tmod_dbs", extension = "rds", contrast="all"),
		map = pph.file_path(step = "tmod_dbs", extension = "mapping.rds", contrast="all")
	output:
		res    = pph.file_path(step = "tmod_pca", extension = "rds", contrast="all"),
		pca    = pph.file_path(step = "tmod_pca", extension = "pca.rds", contrast="all"),
		xlsx   = pph.file_path(step = "tmod_pca", extension = "xlsx", contrast="all")
	log:
		out = pph.file_path(step = "tmod_pca", extension = "output.log", log=True, contrast="all")
	run:
		config_file = pph.file_path(step="pipeline_report", extension="yaml", contrast="all")
		script = textwrap.dedent(r"""
			{R_SESSION_INFO}

			script.d  <- "{SCRIPTDIR}"
			conf.f    <- "{config_file}"
			dds.f     <- "{input.dds}"
			map.f     <- "{input.map}"
			dbs.f     <- "{input.dbs}"
			res.f     <- "{output.res}"
			res.xlsx  <- "{output.xlsx}"
			res.pca.f <- "{output.pca}"
			 
			## ------------------------------
      ## no wildcards beyond this point
			## ------------------------------
			require(DESeq2)
			require(tmod)
			require(tidyverse)
			require(writexl)

			config <- yaml::yaml.load_file(conf.f)
			source(file.path(script.d, "tmod_functions.R"))

			dbs <- readRDS(dbs.f)
			db.map <- readRDS(map.f)
			dds <- readRDS(dds.f)
			rl <- assay(rlog(dds))

			nullvar <- apply(rl, 1, var) < .Machine$double.eps
			rl <- rl[ !nullvar, ]

			genes <- rownames(rl)
			if(is.null(genes)) stop("can't get primary IDs from DESeq2 object!")

		  pca <- prcomp(t(rl), scale.=TRUE)
			saveRDS(pca, file=res.pca.f)

			ncomp <- min(4, ncol(pca$x))

			res <- lapply(dbs, function(db) {{
				mapping.id <- db.map$dbs[[ db$name ]]
				mapping <- db.map$maps[[ mapping.id ]]
				g.id <- setNames(mapping[genes], genes)
				ret <- lapply(1:ncomp, function(i) {{
					glists <- list(
						up=g.id[order(pca$rotation[,i])],
						down=g.id[order(-pca$rotation[,i])],
						abs=g.id[order(-abs(pca$rotation[,i]))])
					lapply(glists, tmodCERNOtest, mset=db$dbobj)
				}})
			  names(ret) <- paste0("PC.", 1:ncomp)
				ret
			}})

			saveRDS(res, file=res.f)

			## shorten results for the human readable XLSX file

			## filter the results
			res.short <- lapply(res, function(x) lapply(x, function(y) lapply(y, reformat_res, pval.thr=.05, auc.thr=.55)))

			## res.short still has two levels, one for the databases, another one
			## for sorting keys, so we need to unlist – one level only
			res.short <- unlist(unlist(res.short, recursive=FALSE), recursive=FALSE)
			write_xlsx(res.short, path=res.xlsx)
		""")

		script_file = pph.log(log.out, snakemake_format(script), step="tmod_pca", contrast="all", extension="R", **wildcards)

		# run R
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")



##-------------------- functional annotation ----------------------------------------------------------------

rule goseq:
	""" over-representation analysis (ORA) with goseq producing GO and KEGG lists """
	input:
		txdb = pph.file_path("TxDb_from_GTF", "sqlite", contrast = "all"),
		res  = pph.file_path("contrast", "rds")
	output:
		go   = pph.file_path(step = "goseq", extension = "go.rds"),
		kegg = pph.file_path(step = "goseq", extension = "kegg.rds")
	log:
		out           = pph.file_path(step = "goseq", extension = "output.log",    log=True),
		contrast_yaml = pph.file_path(step = "goseq", extension = "contrast.yaml", log=True)
	params:
		annot_pkg = config["organism"]["R"]["annotations"],
		fit_pwf   = pph.file_path(step = "goseq", extension = "pwf_fit.pdf")
	run:
		contrast = pph.get_contrast(wildcards.contrast)
		with open(log.contrast_yaml, "w") as f: yaml.dump(contrast, f, default_flow_style=False)
		
		fdr_threshold = contrast["ORA"]["fdr_threshold"]
		AnnotationDbi_prefix = ".".join(params.annot_pkg.split(".")[:-1])
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(goseq)
		library(GenomicFeatures)
		library(AnnotationDbi)
		library({params.annot_pkg})
		
		{R_SESSION_INFO}
		
		#----- load results
		res <- readRDS("{input.res}")
		
		#----- vector for goseq
		assayed.genes <- rownames(res)
		de.genes <- assayed.genes[which(res$padj < {fdr_threshold})]
		gene.vector <- as.integer(assayed.genes %in% de.genes); names(gene.vector) <- assayed.genes
                names(gene.vector) <- sub("\\.[0-9]+", "", names(gene.vector))
		
		#----- get gene lengths
		TxDb <- loadDb(file = "{input.txdb}")
		lengthData <- median(width(transcriptsBy(TxDb, "gene")))[rownames(res)]; names(lengthData) <- rownames(res)
		
		#----- get GO mapping
		en2eg <- as.list({AnnotationDbi_prefix}ENSEMBL2EG); go2eg <- as.list({AnnotationDbi_prefix}GO2ALLEGS)
		eg2go <- split(rep(names(go2eg), lengths(go2eg)), unlist(go2eg)); eg2kegg <- as.list({AnnotationDbi_prefix}PATH)

		grepAnnot <- function(id,mapkeys) unique(unlist(mapkeys[id], use.names=FALSE))
		go <- lapply(en2eg, grepAnnot, eg2go); kegg <- lapply(en2eg, grepAnnot, eg2kegg)
		
		#----- run goseq
		pdf("{params.fit_pwf}")
		pwf <- nullp(gene.vector, bias.data=lengthData)
		dev.off()
		GO <- goseq(pwf, gene2cat=go)
		KEGG <- goseq(pwf, gene2cat=kegg)
		
		GO$over_represented_qvalue <- p.adjust(GO$over_represented_pvalue, method="BH")
		GO$under_represented_qvalue <- p.adjust(GO$under_represented_pvalue, method="BH")
		KEGG$over_represented_qvalue <- p.adjust(KEGG$over_represented_pvalue, method="BH")
		KEGG$under_represented_qvalue <- p.adjust(KEGG$under_represented_pvalue, method="BH")
		
		#----- save as rds
		saveRDS(GO, file="{output.go}"); saveRDS(KEGG, file="{output.kegg}")
		""")
		
		script_file = pph.log(log.out, snakemake_format(script), step="goseq", extension="R", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")
	
	
rule cluster_profiler:
	""" gene set enrichment analysis (GSEA) or over-representation analysis (ORA) with cluster profiler for MSigDb, GO and/or KEGG"""
	input:
		res  = pph.file_path("contrast", "rds")
	output:
		rds = pph.file_path(step = "cluster_profiler", extension = "rds"),
		csv = directory(pph.file_path(step = "cluster_profiler", extension = "csv"))
	log:
		out           = pph.file_path(step = "cluster_profiler", extension = "output.log",    log=True),
		contrast_yaml = pph.file_path(step = "cluster_profiler", extension = "contrast.yaml", log=True)
	params:
		annot_pkg = config["organism"]["R"]["annotations"],
		genus = config["organism"]["genus"],
		csv_basename = "category_"
	run:
		contrast = pph.get_contrast(wildcards.contrast)
		with open(log.contrast_yaml, "w") as f: yaml.dump(contrast, f, default_flow_style=False)
		
		fdr_threshold = contrast["ORA"]["fdr_threshold"]
		AnnotationDbi_prefix = ".".join(params.annot_pkg.split(".")[:-1])
		go_pval = go_qval = kegg_pval = kegg_qval = keggm_pval = keggm_qval = ""
		
		test_annotations = {}
		if "MSigDb" in contrast["cluster_profiler"]:
			if "categories" in contrast["cluster_profiler"]["MSigDb"]:
				test_annotations["MSigDb"] = dict(categories='c({})'.format(", ".join('"'+c+'"' for c in contrast["cluster_profiler"]["MSigDb"]["categories"])))
			else:
				test_annotations["MSigDb"] = dict(categories='c("H","C1","C2","C3","C4","C5","C6","C7")')
			test_annotations["MSigDb"]["type"] = '"'+contrast["cluster_profiler"]["MSigDb"]["type"]+'"' if "type" in contrast["cluster_profiler"]["MSigDb"] else '"gsea"'
		if "GO" in contrast["cluster_profiler"]:
			if "ontologies" in contrast["cluster_profiler"]["GO"]:
				test_annotations["GO"] = dict(categories='c({})'.format(", ".join('"'+c+'"' for c in contrast["cluster_profiler"]["GO"]["ontologies"])))
			else:
				test_annotations["GO"] = dict(categories='c("MF", "BP", "CC")')
			test_annotations["GO"]["type"] = '"'+contrast["cluster_profiler"]["GO"]["type"]+'"' if "type" in contrast["cluster_profiler"]["GO"] else '"gsea"'
			if "pval" in contrast["cluster_profiler"]["GO"]: go_pval = ", pvalueCutoff = {}".format(contrast["cluster_profiler"]["GO"]["pval"])
			if "qval" in contrast["cluster_profiler"]["GO"]: go_qval = ", qvalueCutoff = {}".format(contrast["cluster_profiler"]["GO"]["qval"])
		if "KEGG" in contrast["cluster_profiler"]:
			test_annotations["KEGG"] = dict(categories='c("pathways")')
			test_annotations["KEGG"]["org"]  = '"'+contrast["cluster_profiler"]["KEGG"]["kegg_organism_code"]+'"'
			test_annotations["KEGG"]["type"] = '"'+contrast["cluster_profiler"]["KEGG"]["type"]+'"' if "type" in contrast["cluster_profiler"]["KEGG"] else '"gsea"'
			if "pval" in contrast["cluster_profiler"]["KEGG"]: kegg_pval = ", pvalueCutoff = {}".format(contrast["cluster_profiler"]["KEGG"]["pval"])
			if "qval" in contrast["cluster_profiler"]["KEGG"]: kegg_qval = ", qvalueCutoff = {}".format(contrast["cluster_profiler"]["KEGG"]["qval"])
		if "KEGG_modules" in contrast["cluster_profiler"]:
			test_annotations["KEGG_modules"] = dict(categories='c("modules")')
			test_annotations["KEGG_modules"]["org"]  = '"'+contrast["cluster_profiler"]["KEGG"]["kegg_organism_code"]+'"'
			test_annotations["KEGG_modules"]["type"] = '"'+contrast["cluster_profiler"]["KEGG"]["type"]+'"' if "type" in contrast["cluster_profiler"]["KEGG"] else '"gsea"'
			if "pval" in contrast["cluster_profiler"]["KEGG_modules"]: keggm_pval = ", pvalueCutoff = {}".format(contrast["cluster_profiler"]["KEGG_modules"]["pval"])
			if "qval" in contrast["cluster_profiler"]["KEGG_modules"]: keggm_qval = ", qvalueCutoff = {}".format(contrast["cluster_profiler"]["KEGG_modules"]["qval"])
		test_annot_R = "list({})".format(", ".join("{}=list({})".format(k, ",".join("{}={}".format(k2,v) for k2, v in d.items())) for k,d in test_annotations.items()))
		
		script = textwrap.dedent(r"""
		#----- import packages
		library(clusterProfiler)
		library(GenomicFeatures)
		library(AnnotationDbi)
		library({params.annot_pkg})
		library(msigdbr)
		library(dplyr)
		
		{R_SESSION_INFO}
		
		#----- load results
		res <- readRDS("{input.res}")
		
		#----- gene list GSEA
		use_entries <- !is.na(res$entrez)
		geneList_gsea <- res$log2FoldChange[use_entries]
		names(geneList_gsea) <- as.character(res$entrez[use_entries])
		geneList_gsea <- sort(geneList_gsea, decreasing = TRUE)
		
		#----- gene list ORA
		geneList_ora <- res$entrez[res$padj < {fdr_threshold}]
		geneList_ora <- geneList_ora[!is.na(geneList_ora)]
		geneList_ora <- as.character(geneList_ora)
		
		#----- collect results
		test_annotations <- {test_annot_R}
		tests <- list()
		
		#----- run tests
		totest <- intersect(c("MSigDb", "GO", "KEGG", "KEGG_modules"), names(test_annotations))

		tests <- lapply(totest, function(test) {{
			dir.create(paste0("{output.csv}/", test), recursive=TRUE)
			categories <- test_annotations[[test]]$categories
			#cat_tests <- vector("list", length(categories))
			cat_tests <- lapply(categories, function(cat) {{
				if (test == "MSigDb") msigdb_gs <- msigdbr(species = "{params.genus}", category = cat) %>% dplyr::select(gs_name, entrez_gene)
				if (test_annotations[[test]]$type == "gsea"){{
					em <- switch(test,
					MSigDb        =GSEA(geneList_gsea, TERM2GENE = msigdb_gs),
					GO            =gseGO(geneList = geneList_gsea, OrgDb = {params.annot_pkg}, ont = cat, pAdjustMethod = "BH"{go_pval}),
					KEGG          =gseKEGG(geneList = geneList_gsea, organism = test_annotations$KEGG$org{kegg_pval}),
					KEGG_modules  =gseMKEGG(geneList = geneList_gsea, organism = test_annotations$KEGG_modules$org{keggm_pval}))
				}} else {{
					em <- switch(test,
					MSigDb         =enricher(geneList_ora, TERM2GENE = msigdb_gs),
					GO             =enrichGO(gene = geneList_ora, universe = names(geneList_gsea), OrgDb = {params.annot_pkg}, 
					                         ont = cat, pAdjustMethod = "BH"{go_pval}{go_qval}),
					KEGG           =enrichKEGG(gene = geneList_ora, organism = test_annotations$KEGG$org{kegg_pval}{kegg_qval}),
					KEGG_modules   =enrichMKEGG(gene = geneList_ora, organism = test_annotations$KEGG_modules$org{keggm_pval}{keggm_qval}))
				}}
				write.table(data.frame(em), file=paste0("{output.csv}/",test,"/{params.csv_basename}",cat,".csv"), 
					    sep="\t", col.names=TRUE, row.names=FALSE, quote=FALSE)
				return(em)
			}})
			names(cat_tests) <- categories
			return(cat_tests)
		}})
		names(tests) <- totest
		
		#----- save as rds
		saveRDS(tests, file="{output.rds}")
		""")
		
		script_file = pph.log(log.out, snakemake_format(script), step="cluster_profiler", extension="R", **wildcards)
		shell("Rscript --vanilla '{script_file}' &>> '{log.out}'")

		
##-------------------- QC steps -----------------------------------------------------------------------------

# plots go into report snippets


##-------------------- report -------------------------------------------------------------------------------

rule report:
	""" write a static Rmd report """
	input:
		get_inputs_all()
	output:
		pph.file_path("report", "Rmd", contrast = "all")
	run:
		rt = ReportTool(pph)
		report_text = rt.generate_report()

		text_sub = dict(file_tab="", config="")
		for tag, num, path in ((tag, num, path) for tag, paths in rt.use_results.items() for num, path in enumerate(paths)):
			file_table  = pph.file_path("report",          "tsv",  contrast="all", path_pattern=path)
			config_file = pph.file_path("pipeline_report", "yaml", contrast="all", path_pattern=path)
			pph.log_generated_files(save_to=file_table, path_pattern=path)
			id_suffix, _ = rt.get_id_suffix(tag, num)
			text_sub["file_tab"] += (f'file_tab{id_suffix} <- data.frame(read.table("{file_table}", sep="\\t", header=TRUE))\n'
			                         f'file_tab{id_suffix}$filename <- as.character(file_tab{id_suffix}$filename)\n')
			text_sub["config"]   += f'config{id_suffix} <- yaml.load_file("{config_file}")\n'

		report_text = report_text.replace("{{WORKING_DIRECTORY}}", os.getcwd() + os.sep)
		report_text = report_text.replace("{{R_COMMON}}", str(rt.report_snippet_base_dir/"R_common"))
		report_text = report_text.replace("{{LOAD_FILE_TABLE}}", text_sub["file_tab"])
		report_text = report_text.replace("{{LOAD_CONFIG_FILE}}", text_sub["config"])
		
		with open(output[0], "w") as f: f.write(report_text)


## ----------------------- generate html report -------------------------

rule report_html:
	"""Generate html version of the report"""
	input:
		rmd=pph.file_path("report", "Rmd", contrast="all")
	output:
		html=pph.file_path("report_html", "html", contrast="all")
	log:
		out=pph.file_path(step = "report_html", extension = "output.log", log=True, contrast="all")
	run:
		shell("Rscript --vanilla -e 'rmarkdown::render(\"{input.rmd}\", output_file=\"{output.html}\")' &>> '{log.out}'")


